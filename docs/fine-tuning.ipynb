{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93937930-2426-4cfa-a8fb-83ef6171cb11",
   "metadata": {},
   "source": [
    "## __Statistical and Linguistic Insights for Model Explanation - SLIME__ \n",
    "### __Fine-tuning custom LLM for classification__\n",
    "<font size=3>\n",
    "\n",
    "To improve/modify the LLM for classification, we can modify the $\\mathtt{CustomModel}$ class to make the NN modeling using:\n",
    "* $\\mathtt{FitModel().fit()}$ method when the available dataset is large enough for training and validation;\n",
    "* $\\mathtt{FitModel().kfold()}$ method when the available dataset is small for training and validation.\n",
    "\n",
    "After NN modeling, we make the final training using the $\\mathtt{FitModel().fit()}$ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9cd509e-a6a0-41fb-9171-21574f635a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from slime_nlp.dataset import ImportData\n",
    "from slime_nlp.model import CustomModel, FitModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55b26c-6d6f-4642-8c6d-65f12000af29",
   "metadata": {},
   "source": [
    "### __1. Fine-tuning: for train and validation data__\n",
    "<font size=3>\n",
    "\n",
    "- Using $\\mathtt{ImportData}$ to split the dataset into train, validation, and test data;\n",
    "- Using $\\mathtt{FitModel.fit()}$ for train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d68d6c-1225-4679-974a-64dfeb71e842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "                                                 text  group\n",
      "0  well the little girl is saying to be uiet to h...      0\n",
      "1  mhm . well the water's running over on the flo...      0\n",
      "2  look at the picture <unintelligible> . oh okay...      0\n",
      "\n",
      "Data length: N_total = 156\n",
      "N-train = 118, N-val = 23, N-test = 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id = ImportData(path_name=\"../dataset/adress_all.csv\", n_val=0.15, n_test=0.1, \n",
    "                group_by=['text', 'group'], verbose=True)\n",
    "\n",
    "train_data = id.train\n",
    "val_data = id.val\n",
    "test_data = id.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55f1541-d2d3-4761-bcec-0dcd36e46d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    # FitModel: CustomModel model fitting.\n",
      "\n",
      "    Input: (device='cpu', optimizer='AdamW', lr=2e-5, lr_sub=2e-4, eps=1e-8)\n",
      "    -----\n",
      "    - device (str): select CPU or GPU for training.\n",
      "    - optimizer (str): training optimizer name.\n",
      "    - lr (float): learning-rate for AutoModel' LLM weights adjustment.\n",
      "    - lr_sub (float): learning-rate for weights adjustment of the CustomModel's \n",
      "    additional layer block.\n",
      "    - eps (float): optimizer constant for numerical stability.\n",
      "\n",
      "    Methods:\n",
      "    -------\n",
      "    - train_step (X, y):\n",
      "      -- X (Tensor): CustomModel input data.\n",
      "      -- y (Tensor): tensor of numerical labels.\n",
      "\n",
      "      Returns (Tensor) the loss function value.\n",
      "\n",
      "    - fit (train_data, val_data=None, epochs=1, batch_size=1, pretrained_name=\"google-bert/bert-base-cased\",\n",
      "    klabel='', path_name=None, patience=0, min_delta=1e-2):\n",
      "      -- train_data (Dataframe): pandas dataframe (ImportData's output) with \"text\"(str) \n",
      "      and \"group\"(int) columns.\n",
      "      -- val_data (Dataframe): equivalent to train_data.\n",
      "      -- epochs (int): number of epochs for training.\n",
      "      -- batch_size (int): data batch-size value.\n",
      "      -- pretained_name (str): pretrained model name from huggingface.co repository.\n",
      "      -- klabel (str): string argument for kfold() method.\n",
      "      -- path_name (srt): path and model name string for saving.\n",
      "      -- patience (int): number of epochs to wait for the early-stop mechanism. For\n",
      "      patience=0, the early-stop is not consider.\n",
      "      -- min_delta (float): tolerance value above the best metric result during the training. \n",
      "      If no improvement is achieved, the training the stopped.\n",
      "\n",
      "    - kfold (data, K=2, epochs=1, batch_size=1, model_name=None, pretrained_name=\"google-bert/bert-base-cased\"):\n",
      "      -- data (Dataframe): pandas dataframe (ImportData's output) with \"text\"(str) \n",
      "      and \"group\"(int) columns.\n",
      "      -- K (int): number of folds for cross-validation.\n",
      "      -- epochs (int): number of epochs for each cross-validation loop.\n",
      "      -- batch_size (int): data batch-size value.\n",
      "      -- model_name (srt): path and model name string for saving.\n",
      "      -- pretained_name (str): pretrained model name from huggingface.co repository.\n",
      "\n",
      "    - plot_metric (path_name=None):\n",
      "      -- path_name (srt): path and plot name string for saving.\n",
      "      \n",
      "      Returns the plot of the loss function values and metrics during training.\n",
      "      \n",
      "    - fold_evaluation:\n",
      "      Evaluates the performance of the K-fold cross-validation.\n",
      "\n",
      "    - evaluate (data):\n",
      "      -- data (Dataframe): pandas dataframe (ImportData's output) with \"text\"(str) \n",
      "      and \"group\"(int) columns.\n",
      "\n",
      "      Returns the mean values of the F1-score and Accuracy metrics. \n",
      "\n",
      "    - save (path_name=\"weights/model_weights.pt\"):\n",
      "      -- model_name (srt): path and model name string for saving.\n",
      "\n",
      "      Creates the \"weights\" directory and saves the model's inner parameters.\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fm = FitModel(device='cpu')\n",
    "\n",
    "print(fm.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb680e2-a80d-4cc7-82cd-9299b05ad8d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Epoch 1/1:\n",
      "Batch:99% - <train-loss> = 7.261e-01\n",
      "<validation-metric>: Acc = 5.652e-01, F1 = 0.000e+00\n",
      "Time taken: 68.94s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fm.fit(train_data, val_data, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deee07b-16d2-4638-b5d8-c9d32a67358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.plot_metric() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cfaaf0-c9a5-4463-8196-426d7ec25102",
   "metadata": {},
   "source": [
    "### __2. Fine-tuning: for K-fold cross-validation.__\n",
    "<font size=3>\n",
    "\n",
    "- Using $\\mathtt{ImportData}$ to split the dataset into train and test data;\n",
    "- Using $\\mathtt{FitModel.kfold()}$ for K-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c2dcac7-a92f-4aba-b0f7-88dfd5261c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "                                                 text  group\n",
      "0  well the little girl is saying to be uiet to h...      0\n",
      "1  mhm . well the water's running over on the flo...      0\n",
      "2  look at the picture <unintelligible> . oh okay...      0\n",
      "\n",
      "Data length: N_total = 156\n",
      "N-train = 141, N-val = 0, N-test = 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id = ImportData(path_name=\"../dataset/adress_all.csv\", n_val=0.0, n_test=0.1,\n",
    "                group_by=['text', 'group'], verbose=True)\n",
    "\n",
    "train_data = id.train\n",
    "test_data = id.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e39b3e-57f2-41b6-ae90-1fee8cfff372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fm = FitModel(device='cpu')\n",
    "\n",
    "fm.kfold(train_data, K=5, batch_size=2, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.plot_metric() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853a8fd-5b22-4d02-86ae-1759fd2c6cc5",
   "metadata": {},
   "source": [
    "### __3. Fine-tuning: for final training after NN modeling.__\n",
    "<font size=3>\n",
    "\n",
    "- Using $\\mathtt{ImportData}$ to split the dataset into train and test data;\n",
    "- Using $\\mathtt{FitModel.fit()}$ for final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884d4b15-7d6c-4825-b240-12dda5d17646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "                                                 text  group\n",
      "0  well the little girl is saying to be uiet to h...      0\n",
      "1  mhm . well the water's running over on the flo...      0\n",
      "2  look at the picture <unintelligible> . oh okay...      0\n",
      "\n",
      "Data length: N_total = 156\n",
      "N-train = 141, N-val = 0, N-test = 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id = ImportData(path_name=\"../dataset/adress_all.csv\", n_val=0.0, n_test=0.1,\n",
    "                group_by=['text', 'group'], verbose=True)\n",
    "\n",
    "train_data = id.train\n",
    "test_data = id.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c6ef1-3db3-4b9c-8b35-75bbb1d45ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fm = FitModel(device='cpu')\n",
    "\n",
    "fm.fit(train_data, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621f239-f3a3-4f43-ac48-6fa7e61d3c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943a04d-269d-41f0-a610-6a5131a38e9e",
   "metadata": {},
   "source": [
    "### __4. Making predictions:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426c2696-6a95-4f49-a2f4-f84c9f07d83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    # CustomModel: Custom LLM for classification\n",
      "\n",
      "    Input: (pretrained_name=\"google-bert/bert-base-cased\")\n",
      "    ----- \n",
      "    - pretained_name (str): pretrained model name from huggingface.co repository.\n",
      "\n",
      "    Returns object with callable model's input. \n",
      "\n",
      "    \n",
      "    Methods:\n",
      "    -------\n",
      "    - forward = __call__: (input_ids, token_type_ids=None, attention_mask=None)\n",
      "      -- input_ids (Tensor): sequence of special tokens IDs.\n",
      "      -- token_type_ids (Tensor): sequence of token indices to distinguish \n",
      "      between sentence pairs.\n",
      "      -- attention_mask (Tensor): mask to avoid performing attention on padding \n",
      "      token indices.\n",
      "\n",
      "      Returns a Tensor with linear prediction output.\n",
      "    \n",
      "    - load: (path_name=\"weights/model_weights.pt\", device='cpu') \n",
      "      Loads model's weights.\n",
      "      \n",
      "      -- path_name (str): path string of the model's weights (.pt).\n",
      "      -- device (str): select CPU or GPU for prediction processing.\n",
      "    \n",
      "    - predict: (data)\n",
      "      -- data (Dataframe): pandas dataframe (ImportData's output) with \"text\"(str) \n",
      "      and \"group\"(int) columns.\n",
      "\n",
      "      Returns (Tensor[float]) a tensor with prediction scalars (0, 1).\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "model = CustomModel().to('cpu')\n",
    "\n",
    "print(model.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd729b-bd3a-46ce-963d-65fa84eb1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(\"../weights/best_model.pt\")\n",
    "\n",
    "pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd10cd2-1692-43f8-8441-f28e1093028c",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/v4.45.2/en/model_doc/auto#transformers.AutoConfig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
