
 # First results : =======================================================================================
 # Dataset: __________________________________________________________________________________

 - 149 sentences for k-fold;
 - 7 sentences for testing.


 # The custom Bert classifier: _______________________________________________________________

 - BertModel + dropout layer with probability = 0.1 + ouput linear layer of size = 1.


 # Fit compilation: __________________________________________________________________________

 - Loss function: Binary Cross-Entropy with Logits;

 - Optmizer: AdamW with learning rate (lr) = 2e-5 for BertModel, lr = 2e-4 for output layer, 
 and AdamW's hyperparameter eps = 1e-8.

 - 5-folds for cross-validation: 149%5 = 29 validation data, and 120 training data.

 - Number of epochs = 50

 - Batch_size = 1.


 # Reference:
 - AdamW: 
 Loshchilov, Ilya and Frank Hutter. “Decoupled Weight Decay Regularization.” International Conference on Learning Representations (2017).



 # Last results: ========================================================================================

 · Dataset: __________________________________________________________________________________

 - 146 sentences for k-fold;
 - 10 sentences for testing.


 · The custom Bert classifier: _______________________________________________________________

 - BertModel + dropout layer with probability = 0.1 + ouput linear layer of size = 1.


 # Fit compilation: __________________________________________________________________________

 - Loss function: Binary Cross-Entropy with Logits;

 - Optmizer: AdamW with learning rate (lr) = 1e-5 for BertModel, lr = 1e-3 for output layer, 
 and AdamW's hyperparameter eps = 1e-10.

 - 5-folds for cross-validation: 146%5 = 29 validation data, and 117 training data.

 - Number of epochs = 15

 - Batch_size = 1.

 - Validation scores over K-folds: Acc = (0.69 +- 0.092), F1 = (0.70 +- 0.069)

 1-fold: <train-loss> = 0.12, <val-acc> = 0.61, <val-F1> = 0.66
 2-fold: <train-loss> = 0.14, <val-acc> = 0.66, <val-F1> = 0.68
 3-fold: <train-loss> = 0.15, <val-acc> = 0.81, <val-F1> = 0.80
 4-fold: <train-loss> = 0.18, <val-acc> = 0.77, <val-F1> = 0.74
 5-fold: <train-loss> = 0.32, <val-acc> = 0.62, <val-F1> = 0.63

 - Final training: 
 Accuracy = 0.700
 F1-score = 0.800

